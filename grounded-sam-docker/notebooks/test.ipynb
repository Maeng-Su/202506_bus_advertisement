{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "096dd74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping opencv-python as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: opencv-contrib-python 4.11.0.86\n",
      "Uninstalling opencv-contrib-python-4.11.0.86:\n",
      "  Successfully uninstalled opencv-contrib-python-4.11.0.86\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting opencv-contrib-python==4.7.0.72\n",
      "  Obtaining dependency information for opencv-contrib-python==4.7.0.72 from https://files.pythonhosted.org/packages/fb/89/8370c6864e518be9ca1b54a19b5daf398f4943041e1283ffa7ba0c66c0bd/opencv_contrib_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading opencv_contrib_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-contrib-python==4.7.0.72) (1.22.2)\n",
      "Downloading opencv_contrib_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-contrib-python\n",
      "Successfully installed opencv-contrib-python-4.7.0.72\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall -y opencv-python opencv-contrib-python\n",
    "# !pip install opencv-contrib-python==4.7.0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67ccca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import supervision as sv\n",
    "from groundingdino.util.inference import Model\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 모델 가중치 및 설정 경로 (Dockerfile 기준)\n",
    "GROUNDING_DINO_CONFIG_PATH = \"/app/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "GROUNDING_DINO_CHECKPOINT_PATH = \"/app/groundingdino/gdino_checkpoints/groundingdino_swint_ogc.pth\"\n",
    "SAM_CHECKPOINT_PATH = \"/app/SAM/sam_vit_h_4b8939.pth\"\n",
    "SAM_ENCODER_VERSION = \"vit_h\"\n",
    "\n",
    "# run_grounded_sam.py\n",
    "def grounded_sam(img_path = \"/app/images/KakaoTalk_20250520_140652269.jpg\", text_prompt = \"bus.wheel.window.\", BOX_THRESHOLD = 0.35, TEXT_THRESHOLD = 0.25, output_dir = \"outputs\", save_img=False):\n",
    "    # --- 설정 값 ---\n",
    "\n",
    "    # 출력 폴더 생성\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # --- 모델 로딩 ---\n",
    "    DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # GroundingDINO 모델 로드\n",
    "    grounding_dino_model = Model(\n",
    "        model_config_path=GROUNDING_DINO_CONFIG_PATH, \n",
    "        model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH,\n",
    "        device=str(DEVICE)\n",
    "    )\n",
    "\n",
    "    # SAM 모델 로드\n",
    "    sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_CHECKPOINT_PATH).to(device=DEVICE)\n",
    "    sam_predictor = SamPredictor(sam)\n",
    "\n",
    "    # --- 추론 실행 ---\n",
    "    # 이미지 불러오기\n",
    "    image_bgr = cv2.imread(img_path)\n",
    "    if image_bgr is None:\n",
    "        print(f\"Error: 이미지를 읽을 수 없습니다. 경로를 확인하세요: {img_path}\")\n",
    "        return\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # 1. Grounding DINO로 Bounding Box 탐지\n",
    "    detections = grounding_dino_model.predict_with_classes(\n",
    "        image=image_rgb,\n",
    "        classes=[c.strip() for c in text_prompt.split('.')],\n",
    "        box_threshold=BOX_THRESHOLD,\n",
    "        text_threshold=TEXT_THRESHOLD\n",
    "    )\n",
    "    print(f\"GroundingDINO found {len(detections)} objects.\")\n",
    "\n",
    "    # 2. 탐지된 BBox를 SAM의 입력으로 사용하여 Segmentation Mask 생성\n",
    "    sam_predictor.set_image(image_rgb)\n",
    "    boxes_for_sam = detections.xyxy\n",
    "    \n",
    "    if len(boxes_for_sam) > 0:\n",
    "        masks, _, _ = sam_predictor.predict_torch(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            boxes=torch.tensor(boxes_for_sam, device=sam_predictor.device),\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        detections.mask = masks.cpu().numpy().squeeze(1)\n",
    "    else:\n",
    "        # 객체가 탐지되지 않으면 마스크를 빈 배열로 설정\n",
    "        detections.mask = np.empty((0, *image_bgr.shape[:2]), dtype=bool)\n",
    "\n",
    "    # --- 결과 시각화 및 저장 ---\n",
    "    # Annotator 생성\n",
    "    box_annotator = sv.BoxAnnotator()\n",
    "    mask_annotator = sv.MaskAnnotator()\n",
    "    label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "    # 클래스 레이블 생성\n",
    "    labels = [\n",
    "        f\"{text_prompt.split('.')[class_id]} {confidence:0.2f}\"\n",
    "        for class_id, confidence in zip(detections.class_id, detections.confidence)\n",
    "    ]\n",
    "\n",
    "    annotated_image = None\n",
    "    if save_img==True:\n",
    "        # 원본 이미지에 마스크와 BBox 그리기\n",
    "        annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "        annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)\n",
    "        \n",
    "        # 결과 이미지 저장\n",
    "        output_filename = os.path.basename(img_path)\n",
    "        output_path = os.path.join(output_dir, f\"result_{output_filename}\")\n",
    "        cv2.imwrite(output_path, annotated_image)\n",
    "        \n",
    "        print(f\"결과 이미지가 다음 경로에 저장되었습니다: {output_path}\")\n",
    "\n",
    "    return {\n",
    "        'boxes': boxes_for_sam,\n",
    "        'masks': masks,\n",
    "        'class_ids': detections.class_id,\n",
    "        'confidences': detections.confidence,\n",
    "        'labels': labels,\n",
    "        'annotated_image': annotated_image,\n",
    "        'detections': detections\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63c617dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundingDINO found 3 objects.\n",
      "결과 이미지가 다음 경로에 저장되었습니다: outputs/result_KakaoTalk_20250520_140652269.jpg\n"
     ]
    }
   ],
   "source": [
    "result_grounded_sam = grounded_sam(img_path = \"/app/images/KakaoTalk_20250520_140652269.jpg\", text_prompt = \"bus.wheel.window.\", BOX_THRESHOLD = 0.35, TEXT_THRESHOLD = 0.25, output_dir = \"outputs\", save_img=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b60164a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 3, 3, 3, 3024, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_grounded_sam['boxes']), len(result_grounded_sam['masks']), len(result_grounded_sam['class_ids']), len(result_grounded_sam['confidences']), len(result_grounded_sam['labels']), len(result_grounded_sam['annotated_image']), len(result_grounded_sam['detections'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb384b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_individual_detections(detection_results: dict, output_dir: str, original_image_path: str):\n",
    "    \"\"\"\n",
    "    grounded_sam 함수에서 반환된 각 탐지 결과를 개별 이미지로 시각화하여 저장합니다.\n",
    "\n",
    "    Args:\n",
    "        detection_results (dict): grounded_sam 함수의 반환 값 (boxes, masks, class_ids, etc.).\n",
    "        output_dir (str): 개별 결과 이미지를 저장할 디렉토리 경로입니다.\n",
    "        original_image_path (str): 원본 이미지 파일의 경로입니다.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 원본 이미지 로드 (grounded_sam에서 반환된 BGR 이미지를 사용하는 것이 가장 좋습니다)\n",
    "    image_bgr = detection_results.get('original_image_bgr')\n",
    "    if image_bgr is None:\n",
    "        print(\"Error: 원본 BGR 이미지를 찾을 수 없습니다. 다시 로드합니다.\")\n",
    "        image_bgr = cv2.imread(original_image_path)\n",
    "        if image_bgr is None:\n",
    "            print(f\"Error: 이미지를 읽을 수 없습니다. 경로를 확인하세요: {original_image_path}\")\n",
    "            return\n",
    "\n",
    "    detections = detection_results.get('detections_object')\n",
    "    labels = detection_results.get('labels')\n",
    "\n",
    "    if detections is None or len(detections) == 0:\n",
    "        print(\"탐지된 객체가 없습니다. 개별 이미지를 생성하지 않습니다.\")\n",
    "        return\n",
    "\n",
    "    box_annotator = sv.BoxAnnotator()\n",
    "    mask_annotator = sv.MaskAnnotator()\n",
    "\n",
    "    base_filename = os.path.splitext(os.path.basename(original_image_path))[0]\n",
    "\n",
    "    for i in range(len(detections)):\n",
    "        # SuperGradients Detections 객체는 인덱싱을 통해 개별 탐지를 추출할 수 있습니다.\n",
    "        single_detection = detections[i]\n",
    "        single_label = [labels[i]] # Label needs to be a list for BoxAnnotator\n",
    "\n",
    "        # 원본 이미지 복사본에 현재 탐지 결과만 그립니다.\n",
    "        annotated_single_image = image_bgr.copy()\n",
    "        annotated_single_image = mask_annotator.annotate(scene=annotated_single_image, detections=single_detection)\n",
    "        annotated_single_image = box_annotator.annotate(scene=annotated_single_image, detections=single_detection, labels=single_label)\n",
    "\n",
    "        # 개별 결과 이미지 저장\n",
    "        output_path = os.path.join(output_dir, f\"{base_filename}_detection_{i+1}_{single_label[0].split(' ')[0]}.jpg\")\n",
    "        cv2.imwrite(output_path, annotated_single_image)\n",
    "        print(f\"개별 탐지 결과 이미지가 저장되었습니다: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f6d55ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 모든 탐지 결과 요약 ---\n",
      "탐지된 객체 수: 3\n",
      "  객체 1: bus 0.94 (Box: [ 185.13367 1005.3669  3975.4155  1982.2109 ])\n",
      "  객체 2: wheel 0.66 (Box: [ 962.0431 1660.745  1304.6151 1980.7504])\n",
      "  객체 3: wheel 0.64 (Box: [2937.8667 1666.0944 3274.1938 1982.8295])\n",
      "Error: 원본 BGR 이미지를 찾을 수 없습니다. 다시 로드합니다.\n",
      "탐지된 객체가 없습니다. 개별 이미지를 생성하지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_PATH = \"/app/images/KakaoTalk_20250520_140652269.jpg\"\n",
    "OUTPUT_BASE_DIR = \"outputs\"\n",
    "INDIVIDUAL_OUTPUT_DIR = os.path.join(OUTPUT_BASE_DIR, \"individual_detections\")\n",
    "\n",
    "if result_grounded_sam is not None:\n",
    "    print(\"\\n--- 모든 탐지 결과 요약 ---\")\n",
    "    print(f\"탐지된 객체 수: {len(result_grounded_sam['boxes'])}\")\n",
    "    for i, label in enumerate(result_grounded_sam['labels']):\n",
    "        print(f\"  객체 {i+1}: {label} (Box: {result_grounded_sam['boxes'][i]})\")\n",
    "\n",
    "    # 2. 각 탐지별 이미지 시각화 함수 실행\n",
    "    visualize_individual_detections(result_grounded_sam, INDIVIDUAL_OUTPUT_DIR, IMAGE_PATH)\n",
    "else:\n",
    "    print(\"이미지 처리 실패 또는 탐지된 객체 없음.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1001e932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundingDINO found 3 objects.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m결과 이미지가 다음 경로에 저장되었습니다: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 103\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 86\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m label_annotator \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mLabelAnnotator()\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# 클래스 레이블 생성\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTEXT_PROMPT\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[class_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfidence\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m class_id, confidence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(detections\u001b[38;5;241m.\u001b[39mclass_id, detections\u001b[38;5;241m.\u001b[39mconfidence)\n\u001b[1;32m     89\u001b[0m ]\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# 원본 이미지에 마스크와 BBox 그리기\u001b[39;00m\n\u001b[1;32m     92\u001b[0m annotated_image \u001b[38;5;241m=\u001b[39m mask_annotator\u001b[38;5;241m.\u001b[39mannotate(scene\u001b[38;5;241m=\u001b[39mimage_bgr\u001b[38;5;241m.\u001b[39mcopy(), detections\u001b[38;5;241m=\u001b[39mdetections)\n",
      "Cell \u001b[0;32mIn[22], line 87\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     83\u001b[0m label_annotator \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mLabelAnnotator()\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# 클래스 레이블 생성\u001b[39;00m\n\u001b[1;32m     86\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mTEXT_PROMPT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclass_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfidence\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m class_id, confidence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(detections\u001b[38;5;241m.\u001b[39mclass_id, detections\u001b[38;5;241m.\u001b[39mconfidence)\n\u001b[1;32m     89\u001b[0m ]\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# 원본 이미지에 마스크와 BBox 그리기\u001b[39;00m\n\u001b[1;32m     92\u001b[0m annotated_image \u001b[38;5;241m=\u001b[39m mask_annotator\u001b[38;5;241m.\u001b[39mannotate(scene\u001b[38;5;241m=\u001b[39mimage_bgr\u001b[38;5;241m.\u001b[39mcopy(), detections\u001b[38;5;241m=\u001b[39mdetections)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not NoneType"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import supervision as sv\n",
    "from groundingdino.util.inference import Model\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    # --- 설정 값 ---\n",
    "    # 입력/출력 경로\n",
    "    IMAGE_PATH = \"images/image1.jpg\"  # 분석할 이미지 경로\n",
    "    OUTPUT_DIR = \"outputs\"\n",
    "    \n",
    "    # 모델 파라미터\n",
    "    TEXT_PROMPT = \"person, dog\"  # 탐지할 객체\n",
    "    BOX_THRESHOLD = 0.35\n",
    "    TEXT_THRESHOLD = 0.25\n",
    "\n",
    "    # 모델 가중치 및 설정 경로 (Dockerfile 기준)\n",
    "    GROUNDING_DINO_CONFIG_PATH = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "    GROUNDING_DINO_CHECKPOINT_PATH = \"/app/weights/groundingdino_swint_ogc.pth\"\n",
    "    SAM_CHECKPOINT_PATH = \"/app/weights/sam_vit_h_4b8939.pth\"\n",
    "    SAM_ENCODER_VERSION = \"vit_h\"\n",
    "    \n",
    "    # 출력 폴더 생성\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # --- 모델 로딩 ---\n",
    "    DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # GroundingDINO 모델 로드\n",
    "    grounding_dino_model = Model(\n",
    "        model_config_path=GROUNDING_DINO_CONFIG_PATH, \n",
    "        model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH,\n",
    "        device=str(DEVICE)\n",
    "    )\n",
    "\n",
    "    # SAM 모델 로드\n",
    "    sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_CHECKPOINT_PATH).to(device=DEVICE)\n",
    "    sam_predictor = SamPredictor(sam)\n",
    "\n",
    "    # --- 추론 실행 ---\n",
    "    # 이미지 불러오기\n",
    "    image_bgr = cv2.imread(IMAGE_PATH)\n",
    "    if image_bgr is None:\n",
    "        print(f\"Error: 이미지를 읽을 수 없습니다. 경로를 확인하세요: {IMAGE_PATH}\")\n",
    "        return\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # 1. Grounding DINO로 Bounding Box 탐지\n",
    "    detections = grounding_dino_model.predict_with_classes(\n",
    "        image=image_rgb,\n",
    "        classes=[c.strip() for c in TEXT_PROMPT.split(',')],\n",
    "        box_threshold=BOX_THRESHOLD,\n",
    "        text_threshold=TEXT_THRESHOLD\n",
    "    )\n",
    "    print(f\"GroundingDINO found {len(detections)} objects.\")\n",
    "\n",
    "    # 2. 탐지된 BBox를 SAM의 입력으로 사용하여 Segmentation Mask 생성\n",
    "    sam_predictor.set_image(image_rgb)\n",
    "    boxes_for_sam = detections.xyxy\n",
    "    \n",
    "    if len(boxes_for_sam) > 0:\n",
    "        masks, _, _ = sam_predictor.predict_torch(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            boxes=torch.tensor(boxes_for_sam, device=sam_predictor.device),\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        detections.mask = masks.cpu().numpy().squeeze(1)\n",
    "    else:\n",
    "        # 객체가 탐지되지 않으면 마스크를 빈 배열로 설정\n",
    "        detections.mask = np.empty((0, *image_bgr.shape[:2]), dtype=bool)\n",
    "\n",
    "    # --- 결과 시각화 및 저장 ---\n",
    "    # Annotator 생성\n",
    "    box_annotator = sv.BoxAnnotator()\n",
    "    mask_annotator = sv.MaskAnnotator()\n",
    "    label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "    # 클래스 레이블 생성\n",
    "    labels = [\n",
    "        f\"{TEXT_PROMPT.split(',')[class_id]} {confidence:0.2f}\"\n",
    "        for class_id, confidence in zip(detections.class_id, detections.confidence)\n",
    "    ]\n",
    "\n",
    "    # 원본 이미지에 마스크와 BBox 그리기\n",
    "    annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "    annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections, labels=labels)\n",
    "    \n",
    "    # 결과 이미지 저장\n",
    "    output_filename = os.path.basename(IMAGE_PATH)\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"result_{output_filename}\")\n",
    "    cv2.imwrite(output_path, annotated_image)\n",
    "    \n",
    "    print(f\"결과 이미지가 다음 경로에 저장되었습니다: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e19e841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "from groundingdino.util.inference import Model\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "# Default model paths (modify these if your paths differ)\n",
    "DEFAULT_GDINO_CONFIG = \"/app/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "DEFAULT_GDINO_CHECKPOINT = \"/app/groundingdino/gdino_checkpoints/groundingdino_swint_ogc.pth\"\n",
    "DEFAULT_SAM_CHECKPOINT = \"/app/SAM/sam_vit_h_4b8939.pth\"\n",
    "DEFAULT_SAM_VERSION = \"vit_h\"\n",
    "\n",
    "\n",
    "def run_grounded_sam(\n",
    "    image_path: str,\n",
    "    text_prompt: str,\n",
    "    grounding_dino_config_path: str = DEFAULT_GDINO_CONFIG,\n",
    "    grounding_dino_checkpoint_path: str = DEFAULT_GDINO_CHECKPOINT,\n",
    "    sam_checkpoint_path: str = DEFAULT_SAM_CHECKPOINT,\n",
    "    sam_encoder_version: str = DEFAULT_SAM_VERSION,\n",
    "    box_threshold: float = 0.35,\n",
    "    text_threshold: float = 0.25,\n",
    "    device: str = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Run GroundingDINO + SAM segmentation and bounding box detection on an image.\n",
    "\n",
    "    Args:\n",
    "        image_path: Path to the input image file.\n",
    "        text_prompt: Comma-separated class names to detect (e.g., \"bus, wheel, window\").\n",
    "        grounding_dino_config_path: Path to GroundingDINO config .py file.\n",
    "        grounding_dino_checkpoint_path: Path to GroundingDINO .pth checkpoint.\n",
    "        sam_checkpoint_path: Path to SAM .pth checkpoint.\n",
    "        sam_encoder_version: SAM encoder version key (e.g., 'vit_h', 'vit_l', 'vit_b').\n",
    "        box_threshold: Confidence threshold for bounding boxes.\n",
    "        text_threshold: Text matching threshold in GroundingDINO.\n",
    "        device: Torch device string ('cuda:0' or 'cpu'). If None, auto-selects.\n",
    "\n",
    "    Returns:\n",
    "        dict with the following keys:\n",
    "            'boxes'           : np.ndarray of shape (N, 4) with [x1, y1, x2, y2]\n",
    "            'masks'           : np.ndarray of shape (N, H, W) boolean masks\n",
    "            'class_ids'       : list of int class indices\n",
    "            'confidences'     : list of float confidences\n",
    "            'labels'          : list of str labels with confidence\n",
    "            'annotated_image' : BGR image with drawn boxes & masks\n",
    "            'detections'      : original Detections object for full access\n",
    "    \"\"\"\n",
    "    # Device selection\n",
    "    if device is None:\n",
    "        device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "    print(f\"Using device: {device}\")  # GPU or CPU 확인용 출력\n",
    "\n",
    "    # Load GroundingDINO\n",
    "    grounding_model = Model(\n",
    "        model_config_path=grounding_dino_config_path,\n",
    "        model_checkpoint_path=grounding_dino_checkpoint_path,\n",
    "        device=str(device)\n",
    "    )\n",
    "\n",
    "    # Load SAM\n",
    "    sam = sam_model_registry[sam_encoder_version](checkpoint=sam_checkpoint_path).to(device=device)\n",
    "    sam_predictor = SamPredictor(sam)\n",
    "\n",
    "    # Read image\n",
    "    image_bgr = cv2.imread(image_path)\n",
    "    if image_bgr is None:\n",
    "        raise FileNotFoundError(f\"Cannot read image: {image_path}\")\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # GroundingDINO inference\n",
    "    classes = [c.strip() for c in text_prompt.split(',')]\n",
    "    detections = grounding_model.predict_with_classes(\n",
    "        image=image_rgb,\n",
    "        classes=classes,\n",
    "        box_threshold=box_threshold,\n",
    "        text_threshold=text_threshold\n",
    "    )\n",
    "\n",
    "    # SAM segmentation\n",
    "    sam_predictor.set_image(image_rgb)\n",
    "    boxes_for_sam = detections.xyxy\n",
    "    if len(boxes_for_sam) > 0:\n",
    "        masks, _, _ = sam_predictor.predict_torch(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            boxes=torch.tensor(boxes_for_sam, device=sam_predictor.device),\n",
    "            multimask_output=False\n",
    "        )\n",
    "        masks = masks.cpu().numpy().squeeze(1)\n",
    "    else:\n",
    "        masks = np.empty((0, *image_bgr.shape[:2]), dtype=bool)\n",
    "    detections.mask = masks\n",
    "\n",
    "    # Visualization\n",
    "    mask_annotator = sv.MaskAnnotator()\n",
    "    box_annotator = sv.BoxAnnotator()\n",
    "    annotated = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "    annotated = box_annotator.annotate(scene=annotated, detections=detections)\n",
    "\n",
    "    # Build labels\n",
    "    labels = [f\"{classes[cid]} {conf:0.2f}\" for cid, conf in zip(detections.class_id, detections.confidence)]\n",
    "\n",
    "    return {\n",
    "        'boxes': detections.xyxy,\n",
    "        'masks': masks,\n",
    "        'class_ids': detections.class_id,\n",
    "        'confidences': detections.confidence,\n",
    "        'labels': labels,\n",
    "        'annotated_image': annotated,\n",
    "        'detections': detections\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9b635a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Example usage (in Jupyter or script):\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# from grounded_sam_utils import run_grounded_sam\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_grounded_sam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/app/images/KakaoTalk_20250520_140652269.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbus. wheel. window.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapp/notebooks/outputs/out.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotated_image\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[17], line 101\u001b[0m, in \u001b[0;36mrun_grounded_sam\u001b[0;34m(image_path, text_prompt, grounding_dino_config_path, grounding_dino_checkpoint_path, sam_checkpoint_path, sam_encoder_version, box_threshold, text_threshold, device)\u001b[0m\n\u001b[1;32m     99\u001b[0m mask_annotator \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mMaskAnnotator()\n\u001b[1;32m    100\u001b[0m box_annotator \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mBoxAnnotator()\n\u001b[0;32m--> 101\u001b[0m annotated \u001b[38;5;241m=\u001b[39m \u001b[43mmask_annotator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscene\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_bgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetections\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m annotated \u001b[38;5;241m=\u001b[39m box_annotator\u001b[38;5;241m.\u001b[39mannotate(scene\u001b[38;5;241m=\u001b[39mannotated, detections\u001b[38;5;241m=\u001b[39mdetections)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Build labels\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/supervision/utils/conversion.py:23\u001b[0m, in \u001b[0;36mensure_cv2_image_for_annotation.<locals>.wrapper\u001b[0;34m(self, scene, *args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(annotate_func)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, scene: ImageType, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scene, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m---> 23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mannotate_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscene\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scene, Image\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m     26\u001b[0m         scene_np \u001b[38;5;241m=\u001b[39m pillow_to_cv2(scene)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/supervision/annotators/core.py:365\u001b[0m, in \u001b[0;36mMaskAnnotator.annotate\u001b[0;34m(self, scene, detections, custom_color_lookup)\u001b[0m\n\u001b[1;32m    362\u001b[0m colored_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scene, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m detection_idx \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mflip(np\u001b[38;5;241m.\u001b[39margsort(detections\u001b[38;5;241m.\u001b[39marea)):\n\u001b[0;32m--> 365\u001b[0m     color \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_color\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetection_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetection_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_lookup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor_lookup\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcustom_color_lookup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcustom_color_lookup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m     mask \u001b[38;5;241m=\u001b[39m detections\u001b[38;5;241m.\u001b[39mmask[detection_idx]\n\u001b[1;32m    374\u001b[0m     colored_mask[mask] \u001b[38;5;241m=\u001b[39m color\u001b[38;5;241m.\u001b[39mas_bgr()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/supervision/annotators/utils.py:139\u001b[0m, in \u001b[0;36mresolve_color\u001b[0;34m(color, detections, detection_idx, color_lookup)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve_color\u001b[39m(\n\u001b[1;32m    129\u001b[0m     color: Union[Color, ColorPalette],\n\u001b[1;32m    130\u001b[0m     detections: Detections,\n\u001b[1;32m    131\u001b[0m     detection_idx: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    132\u001b[0m     color_lookup: Union[ColorLookup, np\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m ColorLookup\u001b[38;5;241m.\u001b[39mCLASS,\n\u001b[1;32m    133\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Color:\n\u001b[1;32m    134\u001b[0m     idx \u001b[38;5;241m=\u001b[39m resolve_color_idx(\n\u001b[1;32m    135\u001b[0m         detections\u001b[38;5;241m=\u001b[39mdetections,\n\u001b[1;32m    136\u001b[0m         detection_idx\u001b[38;5;241m=\u001b[39mdetection_idx,\n\u001b[1;32m    137\u001b[0m         color_lookup\u001b[38;5;241m=\u001b[39mcolor_lookup,\n\u001b[1;32m    138\u001b[0m     )\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_color_by_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/supervision/annotators/utils.py:124\u001b[0m, in \u001b[0;36mget_color_by_index\u001b[0;34m(color, idx)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_color_by_index\u001b[39m(color: Union[Color, ColorPalette], idx: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Color:\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(color, ColorPalette):\n\u001b[0;32m--> 124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcolor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mby_idx\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m color\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/supervision/draw/color.py:395\u001b[0m, in \u001b[0;36mColorPalette.by_idx\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mby_idx\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Color:\n\u001b[1;32m    377\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m    Return the color at a given index in the palette.\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m:\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midx argument should not be negative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    397\u001b[0m     idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolors)\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "#Example usage (in Jupyter or script):\n",
    "# from grounded_sam_utils import run_grounded_sam\n",
    "\n",
    "result = run_grounded_sam(\n",
    "    image_path=\"/app/images/KakaoTalk_20250520_140652269.jpg\",\n",
    "    text_prompt=\"bus. wheel. window.\"\n",
    ")\n",
    "cv2.imwrite(\"app/notebooks/outputs/out.png\", result['annotated_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3297de8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 장치: cuda:0\n",
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundingDINO에서 3개의 객체를 찾았습니다.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 122\u001b[0m\n\u001b[1;32m    118\u001b[0m IMAGE_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/app/images/KakaoTalk_20250520_140652269.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m TEXT_PROMPT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbus. wheel. window.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m detected_boxes, detected_masks, detected_labels, detected_class_ids, detected_confidences \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 122\u001b[0m     \u001b[43mprocess_image_with_grounded_sam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMAGE_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTEXT_PROMPT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- 탐지 결과 ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(detected_boxes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m개의 객체를 탐지했습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 94\u001b[0m, in \u001b[0;36mprocess_image_with_grounded_sam\u001b[0;34m(image_path, text_prompt)\u001b[0m\n\u001b[1;32m     92\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_id, confidence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(class_ids, confidences):\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mclass_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_list\u001b[49m\u001b[43m)\u001b[49m: \u001b[38;5;66;03m# class_id가 classes_list 범위 내에 있는지 확인\u001b[39;00m\n\u001b[1;32m     95\u001b[0m         labels\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses_list[class_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfidence\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import supervision as sv\n",
    "from groundingdino.util.inference import Model\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def process_image_with_grounded_sam(image_path: str, text_prompt: str):\n",
    "    \"\"\"\n",
    "    Grounding DINO를 사용하여 객체를 탐지하고 SAM을 사용하여 분할 마스크를 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): 입력 이미지 파일의 경로입니다.\n",
    "        text_prompt (str): 탐지할 객체를 쉼표로 구분한 문자열입니다 (예: \"bus, wheel, window\").\n",
    "\n",
    "    Returns:\n",
    "        tuple: 다음을 포함하는 튜플입니다:\n",
    "            - boxes (list[list[float]]): 각 탐지된 객체의 바운딩 박스 좌표 목록 (x1, y1, x2, y2).\n",
    "            - masks (list[np.ndarray]): 각 탐지된 객체의 이진 분할 마스크 목록.\n",
    "            - labels (list[str]): 각 탐지 결과에 대한 \"클래스_이름 신뢰도_점수\" 형식의 레이블 목록.\n",
    "            - class_ids (list[int]): 각 탐지 결과에 대한 클래스 ID 목록.\n",
    "            - confidences (list[float]): 각 탐지 결과에 대한 신뢰도 점수 목록.\n",
    "    \"\"\"\n",
    "    # --- 설정 값 ---\n",
    "    # 모델 파라미터\n",
    "    BOX_THRESHOLD = 0.35\n",
    "    TEXT_THRESHOLD = 0.25\n",
    "\n",
    "    # 모델 가중치 및 설정 경로 (Dockerfile 기준)\n",
    "    GROUNDING_DINO_CONFIG_PATH = \"/app/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "    GROUNDING_DINO_CHECKPOINT_PATH = \"/app/groundingdino/gdino_checkpoints/groundingdino_swint_ogc.pth\"\n",
    "    SAM_CHECKPOINT_PATH = \"/app/SAM/sam_vit_h_4b8939.pth\"\n",
    "    SAM_ENCODER_VERSION = \"vit_h\"\n",
    "    \n",
    "    # --- 모델 로딩 ---\n",
    "    DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"사용 장치: {DEVICE}\")\n",
    "\n",
    "    # GroundingDINO 모델 로드\n",
    "    grounding_dino_model = Model(\n",
    "        model_config_path=GROUNDING_DINO_CONFIG_PATH, \n",
    "        model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH,\n",
    "        device=str(DEVICE)\n",
    "    )\n",
    "\n",
    "    # SAM 모델 로드\n",
    "    sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_CHECKPOINT_PATH).to(device=DEVICE)\n",
    "    sam_predictor = SamPredictor(sam)\n",
    "\n",
    "    # --- 추론 실행 ---\n",
    "    # 이미지 불러오기\n",
    "    image_bgr = cv2.imread(image_path)\n",
    "    if image_bgr is None:\n",
    "        print(f\"오류: 이미지를 읽을 수 없습니다. 경로를 확인하세요: {image_path}\")\n",
    "        return [], [], [], [], [] # 이미지 로드 실패 시 빈 리스트 반환\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # 1. Grounding DINO로 Bounding Box 탐지\n",
    "    classes_list = [c.strip() for c in text_prompt.split(',')]\n",
    "    detections = grounding_dino_model.predict_with_classes(\n",
    "        image=image_rgb,\n",
    "        classes=classes_list,\n",
    "        box_threshold=BOX_THRESHOLD,\n",
    "        text_threshold=TEXT_THRESHOLD\n",
    "    )\n",
    "    print(f\"GroundingDINO에서 {len(detections)}개의 객체를 찾았습니다.\")\n",
    "\n",
    "    # 2. 탐지된 BBox를 SAM의 입력으로 사용하여 Segmentation Mask 생성\n",
    "    sam_predictor.set_image(image_rgb)\n",
    "    boxes_for_sam = detections.xyxy\n",
    "    \n",
    "    if len(boxes_for_sam) > 0:\n",
    "        masks, _, _ = sam_predictor.predict_torch(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            boxes=torch.tensor(boxes_for_sam, device=sam_predictor.device),\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        detections.mask = masks.cpu().numpy().squeeze(1)\n",
    "    else:\n",
    "        # 객체가 탐지되지 않으면 마스크를 빈 배열로 설정\n",
    "        detections.mask = np.empty((0, *image_bgr.shape[:2]), dtype=bool)\n",
    "\n",
    "    # 반환할 값 준비\n",
    "    boxes = detections.xyxy.tolist() if detections.xyxy is not None else []\n",
    "    masks = detections.mask.tolist() if detections.mask is not None else []\n",
    "    confidences = detections.confidence.tolist() if detections.confidence is not None else []\n",
    "    class_ids = detections.class_id.tolist() if detections.class_id is not None else []\n",
    "\n",
    "    # 출력용 레이블 생성\n",
    "    labels = []\n",
    "    for class_id, confidence in zip(class_ids, confidences):\n",
    "        if class_id < len(classes_list): # class_id가 classes_list 범위 내에 있는지 확인\n",
    "            labels.append(f\"{classes_list[class_id]} {confidence:0.2f}\")\n",
    "        else:\n",
    "            labels.append(f\"UNKNOWN {confidence:0.2f}\") # 범위를 벗어난 class_id에 대한 대체\n",
    "\n",
    "    # --- 결과 시각화 및 저장 (선택 사항 - 디버깅/확인용) ---\n",
    "    # 반환된 데이터만 필요하다면 이 섹션을 주석 처리하거나 제거할 수 있습니다.\n",
    "    OUTPUT_DIR = \"outputs\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    box_annotator = sv.BoxAnnotator()\n",
    "    mask_annotator = sv.MaskAnnotator()\n",
    "    \n",
    "    annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "    annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)\n",
    "    \n",
    "    output_filename = os.path.basename(image_path)\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"result_{output_filename}\")\n",
    "    cv2.imwrite(output_path, annotated_image)\n",
    "    print(f\"결과 이미지가 다음 경로에 저장되었습니다: {output_path}\")\n",
    "\n",
    "    return boxes, masks, labels, class_ids, confidences\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 사용 예시:\n",
    "    IMAGE_PATH = \"/app/images/KakaoTalk_20250520_140652269.jpg\"\n",
    "    TEXT_PROMPT = \"bus. wheel. window.\"\n",
    "\n",
    "    detected_boxes, detected_masks, detected_labels, detected_class_ids, detected_confidences = \\\n",
    "        process_image_with_grounded_sam(IMAGE_PATH, TEXT_PROMPT)\n",
    "\n",
    "    print(\"\\n--- 탐지 결과 ---\")\n",
    "    print(f\"{len(detected_boxes)}개의 객체를 탐지했습니다.\")\n",
    "    for i, (box, mask, label, class_id, confidence) in enumerate(zip(detected_boxes, detected_masks, detected_labels, detected_class_ids, detected_confidences)):\n",
    "        print(f\"객체 {i+1}:\")\n",
    "        print(f\"  레이블: {label}\")\n",
    "        print(f\"  바운딩 박스: {box}\")\n",
    "        print(f\"  마스크 형태: {mask.shape if isinstance(mask, np.ndarray) else 'N/A'}\")\n",
    "        print(f\"  클래스 ID: {class_id}\")\n",
    "        print(f\"  신뢰도: {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8522ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
